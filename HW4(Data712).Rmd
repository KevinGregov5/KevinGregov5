---
title: "HW4"
author: "Kevin Gregov"
date: "2025-03-08"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---
# 1. Reading Assignment
[When adding interaction terms in a logit regression model, interpreting their effects can be tricky because the relationship between variables and the outcome is not straightforward. In a linear regression, an interaction term's coefficient directly shows how two variables combine to affect the outcome. But in a logit model, the effect of an interaction depends on the values of other variables, meaning it can change across observations. Ai and Norton (2003) warn that a significant interaction coefficient does not always mean there is a meaningful impact on the predicted probability. If researchers interpret the interaction term like they would in a linear model, they might mistakenly assume a constant effect when, in reality, it varies across different situations.]{style="font-size: 20px;"}

[A simulation-based approach, as suggested by King, Tomz, and Wittenberg (2000) and Zelner (2009), helps by computing predicted probabilities instead of relying on raw coefficients. By simulating different scenarios, researchers can visualize how an interaction affects the probability of an outcome under various conditions. This makes it easier to see whether the interaction has a strong or weak effect, or if it changes direction depending on the values of other variables. Using simulations helps avoid misinterpretation and provides a clearer, more accurate picture of how interactions work in logit models.]{style="font-size: 20px;"}

# 2. Logit Regression

## DataSet

[The data set I will use to perform a logit regression is the Rossi
dataset from the AER package. The data set contains information on 432
individuals released from prison and tracks whether they were
re-arrested within a specific period.]{style="font-size: 20px;"} <br>

## Key Variables I will analyze

[- Binary Dependent Variable: Rearrested(1 = Yes, 0 =
No)]{style="font-size: 20px;"} <br>

[- Independent Variable: race(black,other)]{style="font-size: 20px;"}
<br>

[- Independent Variable: Marriage(not married,
married)]{style="font-size: 20px;"} <br>

[- Independent Variable: Work Experience(yes,
no)]{style="font-size: 20px;"} <br>

[- Independent Variable: Financial Assistance(yes,
no)]{style="font-size: 20px;"} <br>

## Research Questions

[- Does race influence the likelihood of being
re-arrested?]{style="font-size: 20px;"} <br>

[- Does marriage status impact the likelihood of being
re-arrested?]{style="font-size: 20px;"} <br>

[- Does having work experience reduce the likelihood of
re-arrest?]{style="font-size: 20px;"}

[- Does financial assistance reduce the likelihood of
re-arrest?]{style="font-size: 20px;"}

```{r}
#Start Session
rm(list =ls())
gc()
library(AER)
data(Rossi)
#Tidy Data(Subset Variables and Rename Columns)
library(dplyr)
Rossi <- select(Rossi, arrest, race, mar, wexp, fin) 
Rossi <- rename(Rossi, Rearrested = "arrest", Race = "race", Marriage = "mar", Work.Experience = "wexp", Financial.Assistance = "fin")
head(Rossi)

#Convert my individual-level raw data to grouped data
Grouped <- Rossi %>%
    group_by(Race, Marriage, Work.Experience, Financial.Assistance) %>%
    summarise(total = n(), Yes = sum(Rearrested)) %>%
    mutate(No = total - Yes)
head(Grouped)

#Start Logit Regression
Model_1 <- glm(formula = cbind(Yes, No) ~ Race, family = binomial, data = Grouped)
summary(Model_1)

#Adding two predictors to my model
Model_2 <- glm(formula = cbind(Yes, No) ~ Race + Marriage, family = binomial, data = Grouped) 
summary(Model_2)

#Adding three predictors to my model
Model_3 <- glm(formula = cbind(Yes, No) ~ Race + Marriage + Work.Experience, family = binomial, data = Grouped) 
summary(Model_3)

#Adding four predictors to my model
Model_4 <- glm(formula = cbind(Yes, No) ~ Race + Marriage + Work.Experience + Financial.Assistance, family = binomial, data = Grouped) 
summary(Model_4)
```

```{r, results='asis'}
#Get AIC and BIC values for all models
library(texreg)
htmlreg(list(Model_1, Model_2, Model_3, Model_4), doctype = FALSE)
```

```{r}
# Perform a likelihood ratio test
anova(Model_1, Model_2, Model_3, Model_4, test = "Chisq")

#obtain the probabilities for the predictors that are significant in Model 4.

library(pander)
library(dplyr)

Prob_Table <- Rossi %>% 
    group_by(Work.Experience) %>% 
    summarise(Rearrested = mean(Rearrested)) %>% 
  mutate(Not.Rearrested = 1 - Rearrested) %>% 
  pandoc.table()
 

Prob_Table1 <- Rossi %>% 
    group_by(Financial.Assistance) %>% 
    summarise(Rearrested = mean(Rearrested)) %>% 
  mutate(Not.Rearrested = 1 - Rearrested) %>% 
  pandoc.table()



```
# Interpretation of Results
<br>

## AIC and BIC Values
<br>

```{r echo=FALSE, results='asis'}
library(texreg)
htmlreg(list(Model_1, Model_2, Model_3, Model_4), doctype = FALSE)
```
[By looking at the table you can see that adding more predictors to the model improves the model's fit because the AIC values and BIC values decrease as the numbers of predictors increase. So model 4 is the best fit model because it has the lowest AIC and BIC values compared to the other models.]{style="font-size: 20px;"}
<br>
<br>

# Likelihood Ratio Test

```{r echo=FALSE}
anova(Model_1, Model_2, Model_3, Model_4, test = "Chisq")
```
[Since all the p-values are < 0.05, adding Marriage, Work Experience, and Financial Assistance significantly improves the model's fit. So Model 4 would be the best fit model.]{style="font-size: 20px;"}
<br>
<br>

# Interpretation of Logit Results (Model4)
```{r echo=FALSE}
summary(Model_4)
```

[The Coefficient for Raceother (-0.2205) has a P value of 0.5341, which is greater than 0.05. This means that is no statistically significant relationship between Race and the likelihood of being re-arrested.]{style="font-size: 20px;"}
<br>

[The Coefficient for Marriagenot married (0.2205) has a P value of 0.1583, which is greater than 0.05. This means that is no statistically significant relationship between Marriage and the likelihood of being re-arrested.]{style="font-size: 20px;"}
<br>

[The P value for the Coefficient Work.Experienceyes is .0156, which is less than 0.05. This means that there is a statistically significant relationship between Work Experience and the likelihood of being re - arrested. The Coefficient for Work.Experienceyes is -0.2205, which means that having work experience reduces the likelihood of being re-arrested.]{style="font-size: 20px;"}
<br>

[The P value for the Coefficient of Financial.Assistanceyes is 0.0400 which is less than 0.05. This means that there is a statistically significant relationship between Financial Assistance and the likelihood of being re-arrested. The Coefficient for Financial.Assistanceyes is -0.4598, which means that having financial assistance reduces the likelihood of being re-arrested.]{style="font-size: 20px;"}
<br>

# Probabilities of the likelihood of being re-arrested based on Work Experience and Financial Assistance
```{r echo=FALSE}
Prob_Table <- Rossi %>% 
    group_by(Work.Experience) %>% 
    summarise(Rearrested = mean(Rearrested)) %>% 
  mutate(Not.Rearrested = 1 - Rearrested) %>% 
  pandoc.table()
```
[In my table, individuals with work experience have a lower probability of being rearrested compared to those without work experience.]{style="font-size: 20px;"}
```{r echo=FALSE}
Prob_Table1 <- Rossi %>% 
    group_by(Financial.Assistance) %>% 
    summarise(Rearrested = mean(Rearrested)) %>% 
  mutate(Not.Rearrested = 1 - Rearrested) %>% 
  pandoc.table()
```
[In my table, individuals with financial assistance have a lower probability of being rearrested compared to those without financial assistance.]{style="font-size: 20px;"}
